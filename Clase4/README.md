# Text prediction

On the fourth challenge, we trained a many-to-one neural network model using LSTM layers. The model was trained with jokes found on internet, and the goal is to give an opening (three words) to the model and then it must run itself iteratively to complete the joke one word at step. This is an early basic version of a seq2seq model. The results are bad, we found that the architecture is not good for the case and the dataset is not big enough.

[Link to notebook file](predicci√≥n_chistes.ipynb)
